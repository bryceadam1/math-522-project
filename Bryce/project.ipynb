{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import torch.backends\n",
    "from torch.nn import Module, Linear, ReLU\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.backends.cudnn.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = np.sin\n",
    "\n",
    "x = np.linspace(- np.pi, np.pi, 100)\n",
    "eps = np.random.normal(0, 0.1, 100)\n",
    "\n",
    "y = f(x) + eps\n",
    "\n",
    "plt.scatter(x, y, s=1)\n",
    "plt.show()\n",
    "\n",
    "x_train = x\n",
    "y_train = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Set\n",
    "x = torch.tensor(x, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "y = torch.tensor(y, dtype=torch.float32, device=device).reshape(-1, 1)\n",
    "\n",
    "# Create Data Loader\n",
    "batch_size = 10\n",
    "data = torch.utils.data.TensorDataset(x, y)\n",
    "loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NNModel, self).__init__()\n",
    "        self.linear1 = Linear(input_size, hidden_size)\n",
    "        self.relu = ReLU()\n",
    "        self.linear2 = Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "def initialize(fhat):\n",
    "    # Initialize weights to be close to 0\n",
    "    for param in fhat.parameters():\n",
    "        param.data.normal_(0, 0.01)\n",
    "\n",
    "def train(fhat):\n",
    "    # Training Loop\n",
    "    optimizer = SGD(fhat.parameters(), lr=0.01)\n",
    "    regularization = 0.001\n",
    "    criterion = MSELoss()\n",
    "    epochs = 1000\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            yhat = fhat(x)\n",
    "            loss = criterion(yhat, y) + regularization * (torch.sum(fhat.linear1.weight ** 2) + torch.sum(fhat.linear2.weight ** 2))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch: %d, Loss: %f\" % (epoch, loss.item()))\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return np.array(losses)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 10100 parameters\n",
      "Epoch: 0, Loss: 0.368434\n",
      "Epoch: 100, Loss: 0.082587\n",
      "Epoch: 200, Loss: 0.015407\n",
      "Epoch: 300, Loss: 0.014125\n",
      "Epoch: 400, Loss: 0.042315\n",
      "Epoch: 500, Loss: 0.014256\n",
      "Epoch: 600, Loss: 0.013079\n",
      "Epoch: 700, Loss: 0.013125\n",
      "Epoch: 800, Loss: 0.027240\n",
      "Epoch: 900, Loss: 0.011255\n",
      "\n",
      "Training model with 10200 parameters\n",
      "Epoch: 0, Loss: 0.295427\n",
      "Epoch: 100, Loss: 0.136015\n",
      "Epoch: 200, Loss: 0.036722\n",
      "Epoch: 300, Loss: 0.022598\n",
      "Epoch: 400, Loss: 0.017425\n",
      "Epoch: 500, Loss: 0.026004\n",
      "Epoch: 600, Loss: 0.017278\n",
      "Epoch: 700, Loss: 0.016050\n",
      "Epoch: 800, Loss: 0.012148\n",
      "Epoch: 900, Loss: 0.019947\n",
      "\n",
      "Training model with 10300 parameters\n",
      "Epoch: 0, Loss: 0.350729\n",
      "Epoch: 100, Loss: 0.085278\n",
      "Epoch: 200, Loss: 0.051211\n",
      "Epoch: 300, Loss: 0.024963\n",
      "Epoch: 400, Loss: 0.012565\n",
      "Epoch: 500, Loss: 0.026501\n",
      "Epoch: 600, Loss: 0.035712\n",
      "Epoch: 700, Loss: 0.015629\n",
      "Epoch: 800, Loss: 0.020002\n",
      "Epoch: 900, Loss: 0.013790\n",
      "\n",
      "Training model with 10400 parameters\n",
      "Epoch: 0, Loss: 0.282300\n",
      "Epoch: 100, Loss: 0.034472\n",
      "Epoch: 200, Loss: 0.018729\n",
      "Epoch: 300, Loss: 0.040790\n",
      "Epoch: 400, Loss: 0.026393\n",
      "Epoch: 500, Loss: 0.015967\n",
      "Epoch: 600, Loss: 0.018809\n",
      "Epoch: 700, Loss: 0.017140\n",
      "Epoch: 800, Loss: 0.018770\n",
      "Epoch: 900, Loss: 0.024709\n",
      "\n",
      "Training model with 10500 parameters\n",
      "Epoch: 0, Loss: 0.203379\n",
      "Epoch: 100, Loss: 0.058110\n",
      "Epoch: 200, Loss: 0.058159\n",
      "Epoch: 300, Loss: 0.014167\n",
      "Epoch: 400, Loss: 0.015609\n",
      "Epoch: 500, Loss: 0.014594\n",
      "Epoch: 600, Loss: 0.009617\n",
      "Epoch: 700, Loss: 0.017861\n",
      "Epoch: 800, Loss: 0.017541\n",
      "Epoch: 900, Loss: 0.013613\n",
      "\n",
      "Training model with 10600 parameters\n",
      "Epoch: 0, Loss: 0.313890\n",
      "Epoch: 100, Loss: 0.086109\n",
      "Epoch: 200, Loss: 0.031138\n",
      "Epoch: 300, Loss: 0.017334\n",
      "Epoch: 400, Loss: 0.019165\n",
      "Epoch: 500, Loss: 0.017222\n",
      "Epoch: 600, Loss: 0.026487\n",
      "Epoch: 700, Loss: 0.016032\n",
      "Epoch: 800, Loss: 0.030383\n",
      "Epoch: 900, Loss: 0.011688\n",
      "\n",
      "Training model with 10700 parameters\n",
      "Epoch: 0, Loss: 0.337297\n",
      "Epoch: 100, Loss: 0.034719\n",
      "Epoch: 200, Loss: 0.026785\n",
      "Epoch: 300, Loss: 0.038213\n",
      "Epoch: 400, Loss: 0.025520\n",
      "Epoch: 500, Loss: 0.009115\n",
      "Epoch: 600, Loss: 0.011863\n",
      "Epoch: 700, Loss: 0.016557\n",
      "Epoch: 800, Loss: 0.029874\n",
      "Epoch: 900, Loss: 0.014597\n",
      "\n",
      "Training model with 10800 parameters\n",
      "Epoch: 0, Loss: 0.276156\n",
      "Epoch: 100, Loss: 0.096549\n",
      "Epoch: 200, Loss: 0.025360\n",
      "Epoch: 300, Loss: 0.020085\n",
      "Epoch: 400, Loss: 0.014737\n",
      "Epoch: 500, Loss: 0.020830\n",
      "Epoch: 600, Loss: 0.017384\n",
      "Epoch: 700, Loss: 0.022123\n",
      "Epoch: 800, Loss: 0.014899\n",
      "Epoch: 900, Loss: 0.014094\n",
      "\n",
      "Training model with 10900 parameters\n",
      "Epoch: 0, Loss: 0.402614\n",
      "Epoch: 100, Loss: 0.049747\n",
      "Epoch: 200, Loss: 0.032953\n",
      "Epoch: 300, Loss: 0.014183\n",
      "Epoch: 400, Loss: 0.020891\n",
      "Epoch: 500, Loss: 0.022715\n",
      "Epoch: 600, Loss: 0.015672\n",
      "Epoch: 700, Loss: 0.011559\n",
      "Epoch: 800, Loss: 0.025321\n",
      "Epoch: 900, Loss: 0.014559\n",
      "\n",
      "Training model with 11000 parameters\n",
      "Epoch: 0, Loss: 0.202810\n",
      "Epoch: 100, Loss: 0.096082\n",
      "Epoch: 200, Loss: 0.026597\n",
      "Epoch: 300, Loss: 0.011809\n",
      "Epoch: 400, Loss: 0.033962\n",
      "Epoch: 500, Loss: 0.028322\n",
      "Epoch: 600, Loss: 0.011202\n",
      "Epoch: 700, Loss: 0.015304\n",
      "Epoch: 800, Loss: 0.016104\n",
      "Epoch: 900, Loss: 0.011985\n",
      "\n",
      "Training model with 11100 parameters\n",
      "Epoch: 0, Loss: 0.382128\n",
      "Epoch: 100, Loss: 0.069812\n",
      "Epoch: 200, Loss: 0.024414\n",
      "Epoch: 300, Loss: 0.023971\n",
      "Epoch: 400, Loss: 0.015939\n",
      "Epoch: 500, Loss: 0.014759\n",
      "Epoch: 600, Loss: 0.021039\n",
      "Epoch: 700, Loss: 0.013687\n",
      "Epoch: 800, Loss: 0.011223\n",
      "Epoch: 900, Loss: 0.014471\n",
      "\n",
      "Training model with 11200 parameters\n",
      "Epoch: 0, Loss: 0.332383\n",
      "Epoch: 100, Loss: 0.085602\n",
      "Epoch: 200, Loss: 0.031730\n",
      "Epoch: 300, Loss: 0.015818\n",
      "Epoch: 400, Loss: 0.017092\n",
      "Epoch: 500, Loss: 0.015803\n",
      "Epoch: 600, Loss: 0.024334\n",
      "Epoch: 700, Loss: 0.012943\n",
      "Epoch: 800, Loss: 0.010195\n",
      "Epoch: 900, Loss: 0.020447\n",
      "\n",
      "Training model with 11300 parameters\n",
      "Epoch: 0, Loss: 0.300562\n",
      "Epoch: 100, Loss: 0.075739\n",
      "Epoch: 200, Loss: 0.033589\n",
      "Epoch: 300, Loss: 0.019156\n",
      "Epoch: 400, Loss: 0.020607\n",
      "Epoch: 500, Loss: 0.012693\n",
      "Epoch: 600, Loss: 0.012346\n",
      "Epoch: 700, Loss: 0.013287\n",
      "Epoch: 800, Loss: 0.017392\n",
      "Epoch: 900, Loss: 0.019689\n",
      "\n",
      "Training model with 11400 parameters\n",
      "Epoch: 0, Loss: 0.261788\n",
      "Epoch: 100, Loss: 0.107840\n",
      "Epoch: 200, Loss: 0.036321\n",
      "Epoch: 300, Loss: 0.021147\n",
      "Epoch: 400, Loss: 0.016641\n",
      "Epoch: 500, Loss: 0.021423\n",
      "Epoch: 600, Loss: 0.024370\n",
      "Epoch: 700, Loss: 0.033154\n",
      "Epoch: 800, Loss: 0.018210\n",
      "Epoch: 900, Loss: 0.015193\n",
      "\n",
      "Training model with 11500 parameters\n",
      "Epoch: 0, Loss: 0.477932\n",
      "Epoch: 100, Loss: 0.094421\n",
      "Epoch: 200, Loss: 0.030787\n",
      "Epoch: 300, Loss: 0.021434\n",
      "Epoch: 400, Loss: 0.008805\n",
      "Epoch: 500, Loss: 0.013263\n",
      "Epoch: 600, Loss: 0.012596\n",
      "Epoch: 700, Loss: 0.013318\n",
      "Epoch: 800, Loss: 0.013491\n",
      "Epoch: 900, Loss: 0.023542\n",
      "\n",
      "Training model with 11600 parameters\n",
      "Epoch: 0, Loss: 0.364694\n",
      "Epoch: 100, Loss: 0.095459\n",
      "Epoch: 200, Loss: 0.029139\n",
      "Epoch: 300, Loss: 0.026297\n",
      "Epoch: 400, Loss: 0.014729\n",
      "Epoch: 500, Loss: 0.016699\n",
      "Epoch: 600, Loss: 0.015090\n",
      "Epoch: 700, Loss: 0.036209\n",
      "Epoch: 800, Loss: 0.023060\n",
      "Epoch: 900, Loss: 0.018577\n",
      "\n",
      "Training model with 11700 parameters\n",
      "Epoch: 0, Loss: 0.406298\n",
      "Epoch: 100, Loss: 0.092088\n",
      "Epoch: 200, Loss: 0.011706\n",
      "Epoch: 300, Loss: 0.014384\n",
      "Epoch: 400, Loss: 0.031020\n",
      "Epoch: 500, Loss: 0.027547\n",
      "Epoch: 600, Loss: 0.013767\n",
      "Epoch: 700, Loss: 0.014275\n",
      "Epoch: 800, Loss: 0.025647\n",
      "Epoch: 900, Loss: 0.015978\n",
      "\n",
      "Training model with 11800 parameters\n",
      "Epoch: 0, Loss: 0.279501\n",
      "Epoch: 100, Loss: 0.041339\n",
      "Epoch: 200, Loss: 0.032230\n",
      "Epoch: 300, Loss: 0.015184\n",
      "Epoch: 400, Loss: 0.020134\n",
      "Epoch: 500, Loss: 0.011890\n",
      "Epoch: 600, Loss: 0.019177\n",
      "Epoch: 700, Loss: 0.018273\n",
      "Epoch: 800, Loss: 0.020002\n",
      "Epoch: 900, Loss: 0.013317\n",
      "\n",
      "Training model with 11900 parameters\n",
      "Epoch: 0, Loss: 0.219726\n",
      "Epoch: 100, Loss: 0.080040\n",
      "Epoch: 200, Loss: 0.033607\n",
      "Epoch: 300, Loss: 0.024328\n",
      "Epoch: 400, Loss: 0.011848\n",
      "Epoch: 500, Loss: 0.016945\n",
      "Epoch: 600, Loss: 0.017068\n",
      "Epoch: 700, Loss: 0.023251\n",
      "Epoch: 800, Loss: 0.012730\n",
      "Epoch: 900, Loss: 0.020739\n",
      "\n",
      "Training model with 12000 parameters\n",
      "Epoch: 0, Loss: 0.463906\n",
      "Epoch: 100, Loss: 0.075456\n",
      "Epoch: 200, Loss: 0.017755\n",
      "Epoch: 300, Loss: 0.012047\n",
      "Epoch: 400, Loss: 0.012578\n",
      "Epoch: 500, Loss: 0.013899\n",
      "Epoch: 600, Loss: 0.019331\n",
      "Epoch: 700, Loss: 0.015068\n",
      "Epoch: 800, Loss: 0.012657\n",
      "Epoch: 900, Loss: 0.017868\n",
      "\n",
      "Training model with 12100 parameters\n",
      "Epoch: 0, Loss: 0.405312\n",
      "Epoch: 100, Loss: 0.049254\n",
      "Epoch: 200, Loss: 0.024021\n",
      "Epoch: 300, Loss: 0.015460\n",
      "Epoch: 400, Loss: 0.013411\n",
      "Epoch: 500, Loss: 0.020820\n",
      "Epoch: 600, Loss: 0.020425\n",
      "Epoch: 700, Loss: 0.025490\n",
      "Epoch: 800, Loss: 0.016120\n",
      "Epoch: 900, Loss: 0.023921\n",
      "\n",
      "Training model with 12200 parameters\n",
      "Epoch: 0, Loss: 0.350384\n",
      "Epoch: 100, Loss: 0.067121\n",
      "Epoch: 200, Loss: 0.028148\n",
      "Epoch: 300, Loss: 0.014923\n",
      "Epoch: 400, Loss: 0.013338\n",
      "Epoch: 500, Loss: 0.015179\n",
      "Epoch: 600, Loss: 0.021164\n",
      "Epoch: 700, Loss: 0.014171\n",
      "Epoch: 800, Loss: 0.014904\n",
      "Epoch: 900, Loss: 0.029183\n",
      "\n",
      "Training model with 12300 parameters\n",
      "Epoch: 0, Loss: 0.370369\n",
      "Epoch: 100, Loss: 0.122292\n",
      "Epoch: 200, Loss: 0.022613\n",
      "Epoch: 300, Loss: 0.019778\n",
      "Epoch: 400, Loss: 0.016163\n",
      "Epoch: 500, Loss: 0.015378\n",
      "Epoch: 600, Loss: 0.014708\n",
      "Epoch: 700, Loss: 0.019397\n",
      "Epoch: 800, Loss: 0.022351\n",
      "Epoch: 900, Loss: 0.013246\n",
      "\n",
      "Training model with 12400 parameters\n",
      "Epoch: 0, Loss: 0.355481\n",
      "Epoch: 100, Loss: 0.061092\n",
      "Epoch: 200, Loss: 0.042374\n",
      "Epoch: 300, Loss: 0.012807\n",
      "Epoch: 400, Loss: 0.037993\n",
      "Epoch: 500, Loss: 0.016227\n",
      "Epoch: 600, Loss: 0.017571\n",
      "Epoch: 700, Loss: 0.027164\n",
      "Epoch: 800, Loss: 0.014285\n",
      "Epoch: 900, Loss: 0.016560\n",
      "\n",
      "Training model with 12500 parameters\n",
      "Epoch: 0, Loss: 0.263395\n",
      "Epoch: 100, Loss: 0.056545\n",
      "Epoch: 200, Loss: 0.029424\n",
      "Epoch: 300, Loss: 0.014423\n",
      "Epoch: 400, Loss: 0.036796\n",
      "Epoch: 500, Loss: 0.022883\n",
      "Epoch: 600, Loss: 0.020260\n",
      "Epoch: 700, Loss: 0.010975\n",
      "Epoch: 800, Loss: 0.018682\n",
      "Epoch: 900, Loss: 0.026057\n",
      "\n",
      "Training model with 12600 parameters\n",
      "Epoch: 0, Loss: 0.365113\n",
      "Epoch: 100, Loss: 0.054577\n",
      "Epoch: 200, Loss: 0.016467\n",
      "Epoch: 300, Loss: 0.014865\n",
      "Epoch: 400, Loss: 0.011957\n",
      "Epoch: 500, Loss: 0.013868\n",
      "Epoch: 600, Loss: 0.018792\n",
      "Epoch: 700, Loss: 0.015712\n",
      "Epoch: 800, Loss: 0.019889\n",
      "Epoch: 900, Loss: 0.016524\n",
      "\n",
      "Training model with 12700 parameters\n",
      "Epoch: 0, Loss: 0.390533\n",
      "Epoch: 100, Loss: 0.056788\n",
      "Epoch: 200, Loss: 0.021253\n",
      "Epoch: 300, Loss: 0.011492\n",
      "Epoch: 400, Loss: 0.013645\n",
      "Epoch: 500, Loss: 0.016001\n",
      "Epoch: 600, Loss: 0.025963\n",
      "Epoch: 700, Loss: 0.019405\n",
      "Epoch: 800, Loss: 0.018471\n",
      "Epoch: 900, Loss: 0.013146\n",
      "\n",
      "Training model with 12800 parameters\n",
      "Epoch: 0, Loss: 0.303613\n",
      "Epoch: 100, Loss: 0.037707\n",
      "Epoch: 200, Loss: 0.027913\n",
      "Epoch: 300, Loss: 0.012174\n",
      "Epoch: 400, Loss: 0.011168\n",
      "Epoch: 500, Loss: 0.013572\n",
      "Epoch: 600, Loss: 0.014939\n",
      "Epoch: 700, Loss: 0.017745\n",
      "Epoch: 800, Loss: 0.019591\n",
      "Epoch: 900, Loss: 0.010112\n",
      "\n",
      "Training model with 12900 parameters\n",
      "Epoch: 0, Loss: 0.339814\n",
      "Epoch: 100, Loss: 0.093507\n",
      "Epoch: 200, Loss: 0.056988\n",
      "Epoch: 300, Loss: 0.015547\n",
      "Epoch: 400, Loss: 0.027559\n",
      "Epoch: 500, Loss: 0.012663\n",
      "Epoch: 600, Loss: 0.020104\n",
      "Epoch: 700, Loss: 0.017851\n",
      "Epoch: 800, Loss: 0.020555\n",
      "Epoch: 900, Loss: 0.015743\n",
      "\n",
      "Training model with 13000 parameters\n",
      "Epoch: 0, Loss: 0.129895\n",
      "Epoch: 100, Loss: 0.046694\n",
      "Epoch: 200, Loss: 0.035889\n",
      "Epoch: 300, Loss: 0.013826\n",
      "Epoch: 400, Loss: 0.024359\n",
      "Epoch: 500, Loss: 0.015809\n",
      "Epoch: 600, Loss: 0.014602\n",
      "Epoch: 700, Loss: 0.028350\n",
      "Epoch: 800, Loss: 0.013329\n",
      "Epoch: 900, Loss: 0.026059\n",
      "\n",
      "Training model with 13100 parameters\n",
      "Epoch: 0, Loss: 0.372018\n",
      "Epoch: 100, Loss: 0.056458\n",
      "Epoch: 200, Loss: 0.020509\n",
      "Epoch: 300, Loss: 0.034156\n",
      "Epoch: 400, Loss: 0.017369\n",
      "Epoch: 500, Loss: 0.014018\n",
      "Epoch: 600, Loss: 0.014651\n",
      "Epoch: 700, Loss: 0.010552\n",
      "Epoch: 800, Loss: 0.013281\n",
      "Epoch: 900, Loss: 0.013411\n",
      "\n",
      "Training model with 13200 parameters\n",
      "Epoch: 0, Loss: 0.402234\n",
      "Epoch: 100, Loss: 0.062065\n",
      "Epoch: 200, Loss: 0.022629\n",
      "Epoch: 300, Loss: 0.032415\n",
      "Epoch: 400, Loss: 0.014781\n",
      "Epoch: 500, Loss: 0.012013\n",
      "Epoch: 600, Loss: 0.015417\n",
      "Epoch: 700, Loss: 0.028647\n",
      "Epoch: 800, Loss: 0.025361\n",
      "Epoch: 900, Loss: 0.016535\n",
      "\n",
      "Training model with 13300 parameters\n",
      "Epoch: 0, Loss: 0.392251\n",
      "Epoch: 100, Loss: 0.026182\n",
      "Epoch: 200, Loss: 0.055804\n",
      "Epoch: 300, Loss: 0.011462\n",
      "Epoch: 400, Loss: 0.018818\n",
      "Epoch: 500, Loss: 0.013222\n",
      "Epoch: 600, Loss: 0.010253\n",
      "Epoch: 700, Loss: 0.015048\n",
      "Epoch: 800, Loss: 0.028045\n",
      "Epoch: 900, Loss: 0.015746\n",
      "\n",
      "Training model with 13400 parameters\n",
      "Epoch: 0, Loss: 0.274459\n",
      "Epoch: 100, Loss: 0.140748\n",
      "Epoch: 200, Loss: 0.042955\n",
      "Epoch: 300, Loss: 0.027687\n",
      "Epoch: 400, Loss: 0.024752\n",
      "Epoch: 500, Loss: 0.021536\n",
      "Epoch: 600, Loss: 0.013336\n",
      "Epoch: 700, Loss: 0.017118\n",
      "Epoch: 800, Loss: 0.023929\n",
      "Epoch: 900, Loss: 0.013718\n",
      "\n",
      "Training model with 13500 parameters\n",
      "Epoch: 0, Loss: 0.281898\n",
      "Epoch: 100, Loss: 0.128651\n",
      "Epoch: 200, Loss: 0.029041\n",
      "Epoch: 300, Loss: 0.020350\n",
      "Epoch: 400, Loss: 0.034185\n",
      "Epoch: 500, Loss: 0.018360\n",
      "Epoch: 600, Loss: 0.016207\n",
      "Epoch: 700, Loss: 0.014502\n",
      "Epoch: 800, Loss: 0.021720\n",
      "Epoch: 900, Loss: 0.015226\n",
      "\n",
      "Training model with 13600 parameters\n",
      "Epoch: 0, Loss: 0.396933\n",
      "Epoch: 100, Loss: 0.108854\n",
      "Epoch: 200, Loss: 0.025282\n",
      "Epoch: 300, Loss: 0.034008\n",
      "Epoch: 400, Loss: 0.017086\n",
      "Epoch: 500, Loss: 0.025600\n",
      "Epoch: 600, Loss: 0.010277\n",
      "Epoch: 700, Loss: 0.012509\n",
      "Epoch: 800, Loss: 0.013958\n",
      "Epoch: 900, Loss: 0.016815\n",
      "\n",
      "Training model with 13700 parameters\n",
      "Epoch: 0, Loss: 0.428873\n",
      "Epoch: 100, Loss: 0.059380\n",
      "Epoch: 200, Loss: 0.046615\n",
      "Epoch: 300, Loss: 0.014577\n",
      "Epoch: 400, Loss: 0.012808\n",
      "Epoch: 500, Loss: 0.021183\n",
      "Epoch: 600, Loss: 0.016620\n",
      "Epoch: 700, Loss: 0.018377\n",
      "Epoch: 800, Loss: 0.014440\n",
      "Epoch: 900, Loss: 0.015791\n",
      "\n",
      "Training model with 13800 parameters\n",
      "Epoch: 0, Loss: 0.259117\n",
      "Epoch: 100, Loss: 0.064143\n",
      "Epoch: 200, Loss: 0.039587\n",
      "Epoch: 300, Loss: 0.016479\n",
      "Epoch: 400, Loss: 0.014702\n",
      "Epoch: 500, Loss: 0.011267\n",
      "Epoch: 600, Loss: 0.025260\n",
      "Epoch: 700, Loss: 0.015658\n",
      "Epoch: 800, Loss: 0.026404\n",
      "Epoch: 900, Loss: 0.015780\n",
      "\n",
      "Training model with 13900 parameters\n",
      "Epoch: 0, Loss: 0.246221\n",
      "Epoch: 100, Loss: 0.031900\n",
      "Epoch: 200, Loss: 0.035893\n",
      "Epoch: 300, Loss: 0.015046\n",
      "Epoch: 400, Loss: 0.032766\n",
      "Epoch: 500, Loss: 0.016458\n",
      "Epoch: 600, Loss: 0.019611\n",
      "Epoch: 700, Loss: 0.017791\n",
      "Epoch: 800, Loss: 0.013123\n",
      "Epoch: 900, Loss: 0.022408\n",
      "\n",
      "Training model with 14000 parameters\n",
      "Epoch: 0, Loss: 0.223601\n",
      "Epoch: 100, Loss: 0.070163\n",
      "Epoch: 200, Loss: 0.031012\n",
      "Epoch: 300, Loss: 0.021985\n",
      "Epoch: 400, Loss: 0.027249\n",
      "Epoch: 500, Loss: 0.017384\n",
      "Epoch: 600, Loss: 0.018321\n",
      "Epoch: 700, Loss: 0.015563\n",
      "Epoch: 800, Loss: 0.013543\n",
      "Epoch: 900, Loss: 0.019437\n",
      "\n",
      "Training model with 14100 parameters\n",
      "Epoch: 0, Loss: 0.235700\n",
      "Epoch: 100, Loss: 0.028622\n",
      "Epoch: 200, Loss: 0.020444\n",
      "Epoch: 300, Loss: 0.013258\n",
      "Epoch: 400, Loss: 0.027666\n",
      "Epoch: 500, Loss: 0.017443\n",
      "Epoch: 600, Loss: 0.018401\n",
      "Epoch: 700, Loss: 0.025105\n",
      "Epoch: 800, Loss: 0.012674\n",
      "Epoch: 900, Loss: 0.013687\n",
      "\n",
      "Training model with 14200 parameters\n",
      "Epoch: 0, Loss: 0.258070\n",
      "Epoch: 100, Loss: 0.106581\n",
      "Epoch: 200, Loss: 0.021609\n",
      "Epoch: 300, Loss: 0.026409\n",
      "Epoch: 400, Loss: 0.021472\n",
      "Epoch: 500, Loss: 0.018447\n",
      "Epoch: 600, Loss: 0.013192\n",
      "Epoch: 700, Loss: 0.026835\n",
      "Epoch: 800, Loss: 0.016278\n",
      "Epoch: 900, Loss: 0.014958\n",
      "\n",
      "Training model with 14300 parameters\n",
      "Epoch: 0, Loss: 0.209019\n",
      "Epoch: 100, Loss: 0.031666\n",
      "Epoch: 200, Loss: 0.014017\n",
      "Epoch: 300, Loss: 0.025088\n",
      "Epoch: 400, Loss: 0.021042\n",
      "Epoch: 500, Loss: 0.015696\n",
      "Epoch: 600, Loss: 0.015191\n",
      "Epoch: 700, Loss: 0.016129\n",
      "Epoch: 800, Loss: 0.009758\n",
      "Epoch: 900, Loss: 0.015740\n",
      "\n",
      "Training model with 14400 parameters\n",
      "Epoch: 0, Loss: 0.205219\n",
      "Epoch: 100, Loss: 0.048620\n",
      "Epoch: 200, Loss: 0.047854\n",
      "Epoch: 300, Loss: 0.015536\n",
      "Epoch: 400, Loss: 0.019445\n",
      "Epoch: 500, Loss: 0.014610\n",
      "Epoch: 600, Loss: 0.017346\n",
      "Epoch: 700, Loss: 0.012839\n",
      "Epoch: 800, Loss: 0.014537\n",
      "Epoch: 900, Loss: 0.019757\n",
      "\n",
      "Training model with 14500 parameters\n",
      "Epoch: 0, Loss: 0.299315\n",
      "Epoch: 100, Loss: 0.062797\n",
      "Epoch: 200, Loss: 0.044756\n",
      "Epoch: 300, Loss: 0.032758\n",
      "Epoch: 400, Loss: 0.017860\n",
      "Epoch: 500, Loss: 0.020821\n",
      "Epoch: 600, Loss: 0.027777\n",
      "Epoch: 700, Loss: 0.014136\n",
      "Epoch: 800, Loss: 0.015571\n",
      "Epoch: 900, Loss: 0.016806\n",
      "\n",
      "Training model with 14600 parameters\n",
      "Epoch: 0, Loss: 0.241963\n",
      "Epoch: 100, Loss: 0.049747\n",
      "Epoch: 200, Loss: 0.026543\n",
      "Epoch: 300, Loss: 0.032856\n",
      "Epoch: 400, Loss: 0.011190\n",
      "Epoch: 500, Loss: 0.015641\n",
      "Epoch: 600, Loss: 0.023581\n",
      "Epoch: 700, Loss: 0.012120\n",
      "Epoch: 800, Loss: 0.013425\n",
      "Epoch: 900, Loss: 0.016200\n",
      "\n",
      "Training model with 14700 parameters\n",
      "Epoch: 0, Loss: 0.326147\n",
      "Epoch: 100, Loss: 0.067941\n",
      "Epoch: 200, Loss: 0.027912\n",
      "Epoch: 300, Loss: 0.017053\n",
      "Epoch: 400, Loss: 0.028173\n",
      "Epoch: 500, Loss: 0.018119\n",
      "Epoch: 600, Loss: 0.038551\n",
      "Epoch: 700, Loss: 0.022330\n",
      "Epoch: 800, Loss: 0.020777\n",
      "Epoch: 900, Loss: 0.013658\n",
      "\n",
      "Training model with 14800 parameters\n",
      "Epoch: 0, Loss: 0.384041\n",
      "Epoch: 100, Loss: 0.062466\n",
      "Epoch: 200, Loss: 0.035926\n",
      "Epoch: 300, Loss: 0.027614\n",
      "Epoch: 400, Loss: 0.034888\n",
      "Epoch: 500, Loss: 0.017951\n",
      "Epoch: 600, Loss: 0.011179\n",
      "Epoch: 700, Loss: 0.010710\n",
      "Epoch: 800, Loss: 0.019736\n",
      "Epoch: 900, Loss: 0.012690\n",
      "\n",
      "Training model with 14900 parameters\n",
      "Epoch: 0, Loss: 0.207303\n",
      "Epoch: 100, Loss: 0.082748\n",
      "Epoch: 200, Loss: 0.037898\n",
      "Epoch: 300, Loss: 0.027140\n",
      "Epoch: 400, Loss: 0.023483\n",
      "Epoch: 500, Loss: 0.014947\n",
      "Epoch: 600, Loss: 0.028704\n",
      "Epoch: 700, Loss: 0.016543\n",
      "Epoch: 800, Loss: 0.016614\n",
      "Epoch: 900, Loss: 0.016720\n",
      "\n",
      "Training model with 15000 parameters\n",
      "Epoch: 0, Loss: 0.195450\n",
      "Epoch: 100, Loss: 0.094503\n",
      "Epoch: 200, Loss: 0.029394\n",
      "Epoch: 300, Loss: 0.019318\n",
      "Epoch: 400, Loss: 0.030991\n",
      "Epoch: 500, Loss: 0.015328\n",
      "Epoch: 600, Loss: 0.010427\n",
      "Epoch: 700, Loss: 0.016375\n",
      "Epoch: 800, Loss: 0.021308\n",
      "Epoch: 900, Loss: 0.012710\n",
      "\n",
      "Training model with 15100 parameters\n",
      "Epoch: 0, Loss: 0.276752\n",
      "Epoch: 100, Loss: 0.063252\n",
      "Epoch: 200, Loss: 0.034268\n",
      "Epoch: 300, Loss: 0.015618\n",
      "Epoch: 400, Loss: 0.021047\n",
      "Epoch: 500, Loss: 0.020977\n",
      "Epoch: 600, Loss: 0.016489\n",
      "Epoch: 700, Loss: 0.013273\n",
      "Epoch: 800, Loss: 0.014266\n",
      "Epoch: 900, Loss: 0.024578\n",
      "\n",
      "Training model with 15200 parameters\n",
      "Epoch: 0, Loss: 0.175283\n",
      "Epoch: 100, Loss: 0.050194\n",
      "Epoch: 200, Loss: 0.021093\n",
      "Epoch: 300, Loss: 0.032977\n",
      "Epoch: 400, Loss: 0.015421\n",
      "Epoch: 500, Loss: 0.015427\n",
      "Epoch: 600, Loss: 0.012072\n",
      "Epoch: 700, Loss: 0.029658\n",
      "Epoch: 800, Loss: 0.013433\n",
      "Epoch: 900, Loss: 0.014220\n",
      "\n",
      "Training model with 15300 parameters\n",
      "Epoch: 0, Loss: 0.281366\n",
      "Epoch: 100, Loss: 0.050261\n",
      "Epoch: 200, Loss: 0.019541\n",
      "Epoch: 300, Loss: 0.023134\n",
      "Epoch: 400, Loss: 0.022699\n",
      "Epoch: 500, Loss: 0.012572\n",
      "Epoch: 600, Loss: 0.030283\n",
      "Epoch: 700, Loss: 0.013479\n",
      "Epoch: 800, Loss: 0.013515\n",
      "Epoch: 900, Loss: 0.014902\n",
      "\n",
      "Training model with 15400 parameters\n",
      "Epoch: 0, Loss: 0.234791\n",
      "Epoch: 100, Loss: 0.063103\n",
      "Epoch: 200, Loss: 0.037194\n",
      "Epoch: 300, Loss: 0.018555\n",
      "Epoch: 400, Loss: 0.017182\n",
      "Epoch: 500, Loss: 0.029223\n",
      "Epoch: 600, Loss: 0.012270\n",
      "Epoch: 700, Loss: 0.014482\n",
      "Epoch: 800, Loss: 0.016202\n",
      "Epoch: 900, Loss: 0.015208\n",
      "\n",
      "Training model with 15500 parameters\n",
      "Epoch: 0, Loss: 0.183429\n",
      "Epoch: 100, Loss: 0.036157\n",
      "Epoch: 200, Loss: 0.030440\n",
      "Epoch: 300, Loss: 0.018864\n",
      "Epoch: 400, Loss: 0.010516\n",
      "Epoch: 500, Loss: 0.015222\n",
      "Epoch: 600, Loss: 0.021711\n",
      "Epoch: 700, Loss: 0.011659\n",
      "Epoch: 800, Loss: 0.014969\n",
      "Epoch: 900, Loss: 0.015274\n",
      "\n",
      "Training model with 15600 parameters\n",
      "Epoch: 0, Loss: 0.391429\n",
      "Epoch: 100, Loss: 0.069560\n",
      "Epoch: 200, Loss: 0.022146\n",
      "Epoch: 300, Loss: 0.021671\n",
      "Epoch: 400, Loss: 0.017470\n",
      "Epoch: 500, Loss: 0.017640\n",
      "Epoch: 600, Loss: 0.018944\n",
      "Epoch: 700, Loss: 0.017661\n",
      "Epoch: 800, Loss: 0.014035\n",
      "Epoch: 900, Loss: 0.022422\n",
      "\n",
      "Training model with 15700 parameters\n",
      "Epoch: 0, Loss: 0.240381\n",
      "Epoch: 100, Loss: 0.075601\n",
      "Epoch: 200, Loss: 0.029624\n",
      "Epoch: 300, Loss: 0.016780\n",
      "Epoch: 400, Loss: 0.019384\n",
      "Epoch: 500, Loss: 0.014029\n",
      "Epoch: 600, Loss: 0.014325\n",
      "Epoch: 700, Loss: 0.022300\n",
      "Epoch: 800, Loss: 0.011250\n",
      "Epoch: 900, Loss: 0.018015\n",
      "\n",
      "Training model with 15800 parameters\n",
      "Epoch: 0, Loss: 0.317154\n",
      "Epoch: 100, Loss: 0.066674\n",
      "Epoch: 200, Loss: 0.045085\n",
      "Epoch: 300, Loss: 0.023613\n",
      "Epoch: 400, Loss: 0.024351\n",
      "Epoch: 500, Loss: 0.021418\n",
      "Epoch: 600, Loss: 0.011126\n",
      "Epoch: 700, Loss: 0.020116\n",
      "Epoch: 800, Loss: 0.020111\n",
      "Epoch: 900, Loss: 0.015567\n",
      "\n",
      "Training model with 15900 parameters\n",
      "Epoch: 0, Loss: 0.271049\n",
      "Epoch: 100, Loss: 0.098583\n",
      "Epoch: 200, Loss: 0.036799\n",
      "Epoch: 300, Loss: 0.026944\n",
      "Epoch: 400, Loss: 0.016368\n",
      "Epoch: 500, Loss: 0.015570\n",
      "Epoch: 600, Loss: 0.014426\n",
      "Epoch: 700, Loss: 0.033616\n",
      "Epoch: 800, Loss: 0.016114\n",
      "Epoch: 900, Loss: 0.014149\n",
      "\n",
      "Training model with 16000 parameters\n",
      "Epoch: 0, Loss: 0.192568\n",
      "Epoch: 100, Loss: 0.037158\n",
      "Epoch: 200, Loss: 0.023265\n",
      "Epoch: 300, Loss: 0.017736\n",
      "Epoch: 400, Loss: 0.017732\n",
      "Epoch: 500, Loss: 0.012151\n",
      "Epoch: 600, Loss: 0.015073\n",
      "Epoch: 700, Loss: 0.014627\n",
      "Epoch: 800, Loss: 0.022329\n",
      "Epoch: 900, Loss: 0.014687\n",
      "\n",
      "Training model with 16100 parameters\n",
      "Epoch: 0, Loss: 0.172414\n",
      "Epoch: 100, Loss: 0.039368\n",
      "Epoch: 200, Loss: 0.029182\n",
      "Epoch: 300, Loss: 0.014991\n",
      "Epoch: 400, Loss: 0.013097\n",
      "Epoch: 500, Loss: 0.012509\n",
      "Epoch: 600, Loss: 0.017183\n",
      "Epoch: 700, Loss: 0.020656\n",
      "Epoch: 800, Loss: 0.011317\n",
      "Epoch: 900, Loss: 0.030725\n",
      "\n",
      "Training model with 16200 parameters\n",
      "Epoch: 0, Loss: 0.277608\n",
      "Epoch: 100, Loss: 0.031243\n",
      "Epoch: 200, Loss: 0.029081\n",
      "Epoch: 300, Loss: 0.013771\n",
      "Epoch: 400, Loss: 0.027656\n",
      "Epoch: 500, Loss: 0.019615\n",
      "Epoch: 600, Loss: 0.024171\n",
      "Epoch: 700, Loss: 0.019212\n",
      "Epoch: 800, Loss: 0.019523\n",
      "Epoch: 900, Loss: 0.009794\n",
      "\n",
      "Training model with 16300 parameters\n",
      "Epoch: 0, Loss: 0.362038\n",
      "Epoch: 100, Loss: 0.037894\n",
      "Epoch: 200, Loss: 0.047421\n",
      "Epoch: 300, Loss: 0.034554\n",
      "Epoch: 400, Loss: 0.041764\n",
      "Epoch: 500, Loss: 0.015992\n",
      "Epoch: 600, Loss: 0.013719\n",
      "Epoch: 700, Loss: 0.015501\n",
      "Epoch: 800, Loss: 0.015177\n",
      "Epoch: 900, Loss: 0.016198\n",
      "\n",
      "Training model with 16400 parameters\n",
      "Epoch: 0, Loss: 0.373654\n",
      "Epoch: 100, Loss: 0.119857\n",
      "Epoch: 200, Loss: 0.027876\n",
      "Epoch: 300, Loss: 0.026313\n",
      "Epoch: 400, Loss: 0.036810\n",
      "Epoch: 500, Loss: 0.014142\n",
      "Epoch: 600, Loss: 0.016114\n",
      "Epoch: 700, Loss: 0.014996\n",
      "Epoch: 800, Loss: 0.017536\n",
      "Epoch: 900, Loss: 0.013582\n",
      "\n",
      "Training model with 16500 parameters\n",
      "Epoch: 0, Loss: 0.160528\n",
      "Epoch: 100, Loss: 0.054528\n",
      "Epoch: 200, Loss: 0.042908\n",
      "Epoch: 300, Loss: 0.023241\n",
      "Epoch: 400, Loss: 0.013768\n",
      "Epoch: 500, Loss: 0.013181\n",
      "Epoch: 600, Loss: 0.018479\n",
      "Epoch: 700, Loss: 0.018647\n",
      "Epoch: 800, Loss: 0.020950\n",
      "Epoch: 900, Loss: 0.023016\n",
      "\n",
      "Training model with 16600 parameters\n",
      "Epoch: 0, Loss: 0.307338\n",
      "Epoch: 100, Loss: 0.064364\n",
      "Epoch: 200, Loss: 0.025275\n",
      "Epoch: 300, Loss: 0.018712\n",
      "Epoch: 400, Loss: 0.023583\n",
      "Epoch: 500, Loss: 0.021097\n",
      "Epoch: 600, Loss: 0.017962\n",
      "Epoch: 700, Loss: 0.015607\n",
      "Epoch: 800, Loss: 0.024142\n",
      "Epoch: 900, Loss: 0.012607\n",
      "\n",
      "Training model with 16700 parameters\n",
      "Epoch: 0, Loss: 0.290295\n",
      "Epoch: 100, Loss: 0.045831\n",
      "Epoch: 200, Loss: 0.032986\n",
      "Epoch: 300, Loss: 0.019484\n",
      "Epoch: 400, Loss: 0.031789\n",
      "Epoch: 500, Loss: 0.015469\n",
      "Epoch: 600, Loss: 0.012492\n",
      "Epoch: 700, Loss: 0.020472\n",
      "Epoch: 800, Loss: 0.011791\n",
      "Epoch: 900, Loss: 0.016786\n",
      "\n",
      "Training model with 16800 parameters\n",
      "Epoch: 0, Loss: 0.365371\n",
      "Epoch: 100, Loss: 0.043355\n",
      "Epoch: 200, Loss: 0.021129\n",
      "Epoch: 300, Loss: 0.023350\n",
      "Epoch: 400, Loss: 0.011545\n",
      "Epoch: 500, Loss: 0.012093\n",
      "Epoch: 600, Loss: 0.014822\n",
      "Epoch: 700, Loss: 0.015260\n",
      "Epoch: 800, Loss: 0.012396\n",
      "Epoch: 900, Loss: 0.011069\n",
      "\n",
      "Training model with 16900 parameters\n",
      "Epoch: 0, Loss: 0.263579\n",
      "Epoch: 100, Loss: 0.039712\n",
      "Epoch: 200, Loss: 0.029865\n",
      "Epoch: 300, Loss: 0.021756\n",
      "Epoch: 400, Loss: 0.017679\n",
      "Epoch: 500, Loss: 0.010216\n",
      "Epoch: 600, Loss: 0.017269\n",
      "Epoch: 700, Loss: 0.029240\n",
      "Epoch: 800, Loss: 0.012371\n",
      "Epoch: 900, Loss: 0.009247\n",
      "\n",
      "Training model with 17000 parameters\n",
      "Epoch: 0, Loss: 0.365667\n",
      "Epoch: 100, Loss: 0.036082\n",
      "Epoch: 200, Loss: 0.019055\n",
      "Epoch: 300, Loss: 0.020974\n",
      "Epoch: 400, Loss: 0.013044\n",
      "Epoch: 500, Loss: 0.024930\n",
      "Epoch: 600, Loss: 0.014941\n",
      "Epoch: 700, Loss: 0.019552\n",
      "Epoch: 800, Loss: 0.014506\n",
      "Epoch: 900, Loss: 0.010621\n",
      "\n",
      "Training model with 17100 parameters\n",
      "Epoch: 0, Loss: 0.339260\n",
      "Epoch: 100, Loss: 0.050612\n",
      "Epoch: 200, Loss: 0.018943\n",
      "Epoch: 300, Loss: 0.014856\n",
      "Epoch: 400, Loss: 0.011928\n",
      "Epoch: 500, Loss: 0.026905\n",
      "Epoch: 600, Loss: 0.020082\n",
      "Epoch: 700, Loss: 0.018253\n",
      "Epoch: 800, Loss: 0.024443\n",
      "Epoch: 900, Loss: 0.026422\n",
      "\n",
      "Training model with 17200 parameters\n",
      "Epoch: 0, Loss: 0.320375\n",
      "Epoch: 100, Loss: 0.042462\n",
      "Epoch: 200, Loss: 0.020620\n",
      "Epoch: 300, Loss: 0.014376\n",
      "Epoch: 400, Loss: 0.021144\n",
      "Epoch: 500, Loss: 0.016455\n",
      "Epoch: 600, Loss: 0.011639\n",
      "Epoch: 700, Loss: 0.027734\n",
      "Epoch: 800, Loss: 0.014740\n",
      "Epoch: 900, Loss: 0.031459\n",
      "\n",
      "Training model with 17300 parameters\n",
      "Epoch: 0, Loss: 0.348614\n",
      "Epoch: 100, Loss: 0.040464\n",
      "Epoch: 200, Loss: 0.040178\n",
      "Epoch: 300, Loss: 0.017978\n",
      "Epoch: 400, Loss: 0.018479\n",
      "Epoch: 500, Loss: 0.012804\n",
      "Epoch: 600, Loss: 0.014436\n",
      "Epoch: 700, Loss: 0.026089\n",
      "Epoch: 800, Loss: 0.013208\n",
      "Epoch: 900, Loss: 0.012250\n",
      "\n",
      "Training model with 17400 parameters\n",
      "Epoch: 0, Loss: 0.212757\n",
      "Epoch: 100, Loss: 0.067857\n",
      "Epoch: 200, Loss: 0.014789\n",
      "Epoch: 300, Loss: 0.014701\n",
      "Epoch: 400, Loss: 0.017548\n",
      "Epoch: 500, Loss: 0.014128\n",
      "Epoch: 600, Loss: 0.015645\n",
      "Epoch: 700, Loss: 0.019981\n",
      "Epoch: 800, Loss: 0.018706\n",
      "Epoch: 900, Loss: 0.015267\n",
      "\n",
      "Training model with 17500 parameters\n",
      "Epoch: 0, Loss: 0.401848\n",
      "Epoch: 100, Loss: 0.063569\n",
      "Epoch: 200, Loss: 0.040537\n",
      "Epoch: 300, Loss: 0.011021\n",
      "Epoch: 400, Loss: 0.015975\n",
      "Epoch: 500, Loss: 0.022128\n",
      "Epoch: 600, Loss: 0.015502\n",
      "Epoch: 700, Loss: 0.017665\n",
      "Epoch: 800, Loss: 0.018771\n",
      "Epoch: 900, Loss: 0.013789\n",
      "\n",
      "Training model with 17600 parameters\n",
      "Epoch: 0, Loss: 0.295367\n",
      "Epoch: 100, Loss: 0.026882\n",
      "Epoch: 200, Loss: 0.026211\n",
      "Epoch: 300, Loss: 0.017470\n",
      "Epoch: 400, Loss: 0.030331\n",
      "Epoch: 500, Loss: 0.033605\n",
      "Epoch: 600, Loss: 0.020067\n",
      "Epoch: 700, Loss: 0.013422\n",
      "Epoch: 800, Loss: 0.019829\n",
      "Epoch: 900, Loss: 0.018507\n",
      "\n",
      "Training model with 17700 parameters\n",
      "Epoch: 0, Loss: 0.258534\n",
      "Epoch: 100, Loss: 0.101627\n",
      "Epoch: 200, Loss: 0.034236\n",
      "Epoch: 300, Loss: 0.015690\n",
      "Epoch: 400, Loss: 0.020810\n",
      "Epoch: 500, Loss: 0.018541\n",
      "Epoch: 600, Loss: 0.019618\n",
      "Epoch: 700, Loss: 0.025962\n",
      "Epoch: 800, Loss: 0.014251\n",
      "Epoch: 900, Loss: 0.018676\n",
      "\n",
      "Training model with 17800 parameters\n",
      "Epoch: 0, Loss: 0.285150\n",
      "Epoch: 100, Loss: 0.036659\n",
      "Epoch: 200, Loss: 0.028176\n",
      "Epoch: 300, Loss: 0.025971\n",
      "Epoch: 400, Loss: 0.015011\n",
      "Epoch: 500, Loss: 0.016018\n",
      "Epoch: 600, Loss: 0.021872\n",
      "Epoch: 700, Loss: 0.020230\n",
      "Epoch: 800, Loss: 0.013882\n",
      "Epoch: 900, Loss: 0.010807\n",
      "\n",
      "Training model with 17900 parameters\n",
      "Epoch: 0, Loss: 0.176478\n",
      "Epoch: 100, Loss: 0.031221\n",
      "Epoch: 200, Loss: 0.020139\n",
      "Epoch: 300, Loss: 0.014125\n",
      "Epoch: 400, Loss: 0.013339\n",
      "Epoch: 500, Loss: 0.030426\n",
      "Epoch: 600, Loss: 0.016560\n",
      "Epoch: 700, Loss: 0.017286\n",
      "Epoch: 800, Loss: 0.016872\n",
      "Epoch: 900, Loss: 0.014987\n",
      "\n",
      "Training model with 18000 parameters\n",
      "Epoch: 0, Loss: 0.251650\n",
      "Epoch: 100, Loss: 0.046127\n",
      "Epoch: 200, Loss: 0.038262\n",
      "Epoch: 300, Loss: 0.013645\n",
      "Epoch: 400, Loss: 0.016720\n",
      "Epoch: 500, Loss: 0.022106\n",
      "Epoch: 600, Loss: 0.012346\n",
      "Epoch: 700, Loss: 0.025794\n",
      "Epoch: 800, Loss: 0.020943\n",
      "Epoch: 900, Loss: 0.013626\n",
      "\n",
      "Training model with 18100 parameters\n",
      "Epoch: 0, Loss: 0.277088\n",
      "Epoch: 100, Loss: 0.044083\n",
      "Epoch: 200, Loss: 0.034653\n",
      "Epoch: 300, Loss: 0.027597\n",
      "Epoch: 400, Loss: 0.014405\n",
      "Epoch: 500, Loss: 0.031107\n",
      "Epoch: 600, Loss: 0.013386\n",
      "Epoch: 700, Loss: 0.016539\n",
      "Epoch: 800, Loss: 0.022752\n",
      "Epoch: 900, Loss: 0.012758\n",
      "\n",
      "Training model with 18200 parameters\n",
      "Epoch: 0, Loss: 0.212659\n",
      "Epoch: 100, Loss: 0.052401\n",
      "Epoch: 200, Loss: 0.024534\n",
      "Epoch: 300, Loss: 0.011463\n",
      "Epoch: 400, Loss: 0.029038\n",
      "Epoch: 500, Loss: 0.024391\n",
      "Epoch: 600, Loss: 0.013572\n",
      "Epoch: 700, Loss: 0.017843\n",
      "Epoch: 800, Loss: 0.020681\n",
      "Epoch: 900, Loss: 0.014249\n",
      "\n",
      "Training model with 18300 parameters\n",
      "Epoch: 0, Loss: 0.264141\n",
      "Epoch: 100, Loss: 0.027880\n",
      "Epoch: 200, Loss: 0.021862\n",
      "Epoch: 300, Loss: 0.041136\n",
      "Epoch: 400, Loss: 0.014988\n",
      "Epoch: 500, Loss: 0.022640\n",
      "Epoch: 600, Loss: 0.018729\n",
      "Epoch: 700, Loss: 0.016090\n",
      "Epoch: 800, Loss: 0.026553\n",
      "Epoch: 900, Loss: 0.015064\n",
      "\n",
      "Training model with 18400 parameters\n",
      "Epoch: 0, Loss: 0.222557\n",
      "Epoch: 100, Loss: 0.037899\n",
      "Epoch: 200, Loss: 0.028267\n",
      "Epoch: 300, Loss: 0.038987\n",
      "Epoch: 400, Loss: 0.014030\n",
      "Epoch: 500, Loss: 0.036922\n",
      "Epoch: 600, Loss: 0.017391\n",
      "Epoch: 700, Loss: 0.017543\n",
      "Epoch: 800, Loss: 0.015241\n",
      "Epoch: 900, Loss: 0.014231\n",
      "\n",
      "Training model with 18500 parameters\n",
      "Epoch: 0, Loss: 0.213013\n",
      "Epoch: 100, Loss: 0.061418\n",
      "Epoch: 200, Loss: 0.034898\n",
      "Epoch: 300, Loss: 0.022247\n",
      "Epoch: 400, Loss: 0.023337\n",
      "Epoch: 500, Loss: 0.014722\n",
      "Epoch: 600, Loss: 0.021234\n",
      "Epoch: 700, Loss: 0.012461\n",
      "Epoch: 800, Loss: 0.016446\n",
      "Epoch: 900, Loss: 0.023325\n",
      "\n",
      "Training model with 18600 parameters\n",
      "Epoch: 0, Loss: 0.251303\n",
      "Epoch: 100, Loss: 0.075606\n",
      "Epoch: 200, Loss: 0.017584\n",
      "Epoch: 300, Loss: 0.022255\n",
      "Epoch: 400, Loss: 0.014576\n",
      "Epoch: 500, Loss: 0.015993\n",
      "Epoch: 600, Loss: 0.029722\n",
      "Epoch: 700, Loss: 0.017818\n",
      "Epoch: 800, Loss: 0.012792\n",
      "Epoch: 900, Loss: 0.024463\n",
      "\n",
      "Training model with 18700 parameters\n",
      "Epoch: 0, Loss: 0.208042\n",
      "Epoch: 100, Loss: 0.040380\n",
      "Epoch: 200, Loss: 0.048578\n",
      "Epoch: 300, Loss: 0.018669\n",
      "Epoch: 400, Loss: 0.012577\n",
      "Epoch: 500, Loss: 0.014909\n",
      "Epoch: 600, Loss: 0.021931\n",
      "Epoch: 700, Loss: 0.021440\n",
      "Epoch: 800, Loss: 0.017142\n",
      "Epoch: 900, Loss: 0.014311\n",
      "\n",
      "Training model with 18800 parameters\n",
      "Epoch: 0, Loss: 0.314007\n",
      "Epoch: 100, Loss: 0.069740\n",
      "Epoch: 200, Loss: 0.010625\n",
      "Epoch: 300, Loss: 0.020134\n",
      "Epoch: 400, Loss: 0.018245\n",
      "Epoch: 500, Loss: 0.020872\n",
      "Epoch: 600, Loss: 0.015351\n",
      "Epoch: 700, Loss: 0.015016\n",
      "Epoch: 800, Loss: 0.017074\n",
      "Epoch: 900, Loss: 0.015926\n",
      "\n",
      "Training model with 18900 parameters\n",
      "Epoch: 0, Loss: 0.269359\n",
      "Epoch: 100, Loss: 0.097622\n",
      "Epoch: 200, Loss: 0.021501\n",
      "Epoch: 300, Loss: 0.014274\n",
      "Epoch: 400, Loss: 0.023720\n",
      "Epoch: 500, Loss: 0.020432\n",
      "Epoch: 600, Loss: 0.017174\n",
      "Epoch: 700, Loss: 0.016668\n",
      "Epoch: 800, Loss: 0.016886\n",
      "Epoch: 900, Loss: 0.026886\n",
      "\n",
      "Training model with 19000 parameters\n",
      "Epoch: 0, Loss: 0.441949\n",
      "Epoch: 100, Loss: 0.039225\n",
      "Epoch: 200, Loss: 0.013951\n",
      "Epoch: 300, Loss: 0.027425\n",
      "Epoch: 400, Loss: 0.018575\n",
      "Epoch: 500, Loss: 0.024736\n",
      "Epoch: 600, Loss: 0.016908\n",
      "Epoch: 700, Loss: 0.013556\n",
      "Epoch: 800, Loss: 0.038970\n",
      "Epoch: 900, Loss: 0.021625\n",
      "\n",
      "Training model with 19100 parameters\n",
      "Epoch: 0, Loss: 0.370822\n",
      "Epoch: 100, Loss: 0.050387\n",
      "Epoch: 200, Loss: 0.041059\n",
      "Epoch: 300, Loss: 0.022983\n",
      "Epoch: 400, Loss: 0.015589\n",
      "Epoch: 500, Loss: 0.020181\n",
      "Epoch: 600, Loss: 0.018607\n",
      "Epoch: 700, Loss: 0.018759\n",
      "Epoch: 800, Loss: 0.013207\n",
      "Epoch: 900, Loss: 0.020524\n",
      "\n",
      "Training model with 19200 parameters\n",
      "Epoch: 0, Loss: 0.219069\n",
      "Epoch: 100, Loss: 0.037035\n",
      "Epoch: 200, Loss: 0.022791\n",
      "Epoch: 300, Loss: 0.011279\n",
      "Epoch: 400, Loss: 0.021802\n",
      "Epoch: 500, Loss: 0.020472\n",
      "Epoch: 600, Loss: 0.018974\n",
      "Epoch: 700, Loss: 0.018530\n",
      "Epoch: 800, Loss: 0.013743\n",
      "Epoch: 900, Loss: 0.013642\n",
      "\n",
      "Training model with 19300 parameters\n",
      "Epoch: 0, Loss: 0.173970\n",
      "Epoch: 100, Loss: 0.030766\n",
      "Epoch: 200, Loss: 0.025838\n",
      "Epoch: 300, Loss: 0.020745\n",
      "Epoch: 400, Loss: 0.023273\n",
      "Epoch: 500, Loss: 0.014371\n",
      "Epoch: 600, Loss: 0.026646\n",
      "Epoch: 700, Loss: 0.012142\n",
      "Epoch: 800, Loss: 0.024104\n",
      "Epoch: 900, Loss: 0.015399\n",
      "\n",
      "Training model with 19400 parameters\n",
      "Epoch: 0, Loss: 0.264167\n",
      "Epoch: 100, Loss: 0.046079\n",
      "Epoch: 200, Loss: 0.015815\n",
      "Epoch: 300, Loss: 0.022798\n",
      "Epoch: 400, Loss: 0.030831\n",
      "Epoch: 500, Loss: 0.015215\n",
      "Epoch: 600, Loss: 0.019020\n",
      "Epoch: 700, Loss: 0.012873\n",
      "Epoch: 800, Loss: 0.016230\n",
      "Epoch: 900, Loss: 0.025176\n",
      "\n",
      "Training model with 19500 parameters\n",
      "Epoch: 0, Loss: 0.415625\n",
      "Epoch: 100, Loss: 0.028087\n",
      "Epoch: 200, Loss: 0.038854\n",
      "Epoch: 300, Loss: 0.018977\n",
      "Epoch: 400, Loss: 0.026534\n",
      "Epoch: 500, Loss: 0.019748\n",
      "Epoch: 600, Loss: 0.014482\n",
      "Epoch: 700, Loss: 0.019844\n",
      "Epoch: 800, Loss: 0.015909\n",
      "Epoch: 900, Loss: 0.013261\n",
      "\n",
      "Training model with 19600 parameters\n",
      "Epoch: 0, Loss: 0.286744\n",
      "Epoch: 100, Loss: 0.046400\n",
      "Epoch: 200, Loss: 0.043338\n",
      "Epoch: 300, Loss: 0.025424\n",
      "Epoch: 400, Loss: 0.019771\n",
      "Epoch: 500, Loss: 0.019149\n",
      "Epoch: 600, Loss: 0.021951\n",
      "Epoch: 700, Loss: 0.014420\n",
      "Epoch: 800, Loss: 0.022565\n",
      "Epoch: 900, Loss: 0.019117\n",
      "\n",
      "Training model with 19700 parameters\n",
      "Epoch: 0, Loss: 0.431542\n",
      "Epoch: 100, Loss: 0.056722\n",
      "Epoch: 200, Loss: 0.033649\n",
      "Epoch: 300, Loss: 0.031596\n",
      "Epoch: 400, Loss: 0.022096\n",
      "Epoch: 500, Loss: 0.013988\n",
      "Epoch: 600, Loss: 0.017555\n",
      "Epoch: 700, Loss: 0.013666\n",
      "Epoch: 800, Loss: 0.024102\n",
      "Epoch: 900, Loss: 0.013767\n",
      "\n",
      "Training model with 19800 parameters\n",
      "Epoch: 0, Loss: 0.226047\n",
      "Epoch: 100, Loss: 0.054700\n",
      "Epoch: 200, Loss: 0.021550\n",
      "Epoch: 300, Loss: 0.025686\n",
      "Epoch: 400, Loss: 0.013683\n",
      "Epoch: 500, Loss: 0.018630\n",
      "Epoch: 600, Loss: 0.018474\n",
      "Epoch: 700, Loss: 0.018544\n",
      "Epoch: 800, Loss: 0.014663\n",
      "Epoch: 900, Loss: 0.026426\n",
      "\n",
      "Training model with 19900 parameters\n",
      "Epoch: 0, Loss: 0.245726\n",
      "Epoch: 100, Loss: 0.034674\n",
      "Epoch: 200, Loss: 0.019750\n",
      "Epoch: 300, Loss: 0.037048\n",
      "Epoch: 400, Loss: 0.023068\n",
      "Epoch: 500, Loss: 0.032003\n",
      "Epoch: 600, Loss: 0.017529\n",
      "Epoch: 700, Loss: 0.013041\n",
      "Epoch: 800, Loss: 0.020441\n",
      "Epoch: 900, Loss: 0.011338\n",
      "\n",
      "Training model with 20000 parameters\n",
      "Epoch: 0, Loss: 0.221946\n",
      "Epoch: 100, Loss: 0.041168\n",
      "Epoch: 200, Loss: 0.026117\n",
      "Epoch: 300, Loss: 0.027376\n",
      "Epoch: 400, Loss: 0.016779\n",
      "Epoch: 500, Loss: 0.016944\n",
      "Epoch: 600, Loss: 0.028001\n",
      "Epoch: 700, Loss: 0.017173\n",
      "Epoch: 800, Loss: 0.018760\n",
      "Epoch: 900, Loss: 0.015689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_params = np.arange(100, 20001, 100)\n",
    "\n",
    "for n in n_params:\n",
    "    print(f\"Training model with {n} parameters\")\n",
    "    fhat = NNModel(1, n, 1).to(device)\n",
    "    initialize(fhat)\n",
    "    loss = train(fhat)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(fhat.state_dict(), f\"models/model_{n}.pt\")\n",
    "\n",
    "    # Save the loss\n",
    "    np.save(f\"models/loss_{n}.npy\", loss)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
